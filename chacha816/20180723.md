# 20180723

## <오전수업>

### 1. 분석 Pandas

```
- 엑셀보다 불러올 수 있는 파일의 사이즈가 크고, 더 다양하게 분석할 수 있는 tool
```



### 2. 수치계산 Numpy

```
- 데이터 구조외에도 수치계산을 위해 효율적으로 구현 된 기능을 제공
```



### 3. 시각화

```
Matplolib
  - plotline
  - seaborn
```



### 4. scikit learn

#### 1) classification(분류)

#### 2) regression(회귀)

#### 3) clustering(집단화) 

#### 4) dimensionality reduction(차원감소)



-----

### Supervised Machine Learning (지도학습)

* clf.fit(X_train, y_train)

```
X는 대문자를 사용하고, y는 소문자를 사용하는 이유는 
X = 행렬   y는 벡터이기 때문이다.
```
```
X_train은 train데이터의 행렬을 뜻한다.
y_train은 train데이터의 벡터를 뜻한다.
```



* clf.score(X_test, y_test)

```
train을 통해 알고싶은 것이나, 확인한 결과가 맞는지
test를 통해 알아본다.
```





### Unsupervised Machine Learning (비지도학습)

* 지도학습과 차이점?

  ```
  지도학습은 레이블이 있고, 비지도학습은 레이블이 없다.
  (기준을 주면 지도학습  /  기준이 없으면 비지도학습)
  ```

  ```
  분류할 때 clustering을 하면 Unsupervised Machine Learning 이다.
  ```

​:gift_heart:  정지은님 설명 :gift_heart:

#### 몸무게와 키를 feature라고 하고 분류할 때 label이 개/고양이 중 무엇인지 찾는 것 

```
몸무게     키      label
30kg      20cm      개
20kg      15cm    고양이
```
###### (지은님 감사해요! 이해가 한번에 됬어요>3<)



### Overfitting(과대적합)과 Underfitting(과소적합)

#### 1)Overfirring 

```
모델이 훈련 데이터에 너무 잘 맞지만 일반성이 떨어진다는 의미
-> 너무 훈련데이터에 맞춰져 있어서 테스트 데이터에서는 높은 성능을 보여줄 확률이 낮다.
-> 모델의 복잡도가 필요이상으로 높기 때문이다.
```

* 해결방법

    ```
    - 훈련 데이터를 더 많이 모은다.
    - 정규화
    - 오류 수정과 이상치 제거
    ```


#### 2) Underfitting

```
모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할 때 발생.
-> 모수가 너무 작아서 발생
```

* 해결방법
  ```
  - 파라미터가 더 많은 복잡한 모델을 선택
  - 모델의 제약을 줄이기(규제 하이퍼파라미터 값 줄이기)
  ```



-----

### 5. soynlp 실습

```
text = re.sub('\\\\n', ' ', text)에서 공백을 설정해주는 이유는 단어끼리 붙는것을 방지하기 위해서 이다.
```

```
%time을 입력하면 해당 코드를 실행할 때 걸리는 시간을 출력해준다.
%%time = 해당줄이 여러개일 때 사용한다.
```

```
def remove_stopwords(text):
    stops = ['수', '있는', '있습니다', '그', '년도', '에', '합니다', '하는', '및', '제', '할', '하고', '더', '대한', '한', '그리고', '월', '저는', '없는', '것입니다', '등', '일', '많은', '이런', '것은', '왜', '같은', '없습니다', '위해', '한다']
    # Stopwords 불용어 제거
    meaningful_words = [word for word in text if not word in stops]
    return ' '.join(meaningful_words)

%time tokens_remove_stopwords = 토큰으로만든단어들.apply(remove_stopwords)
```

```
def preprocessing(text):
    # 개행문자 제거
    text = re.sub('\\\\n', ' ', text)
    # 특수문자 제거
    # 특수문자나 이모티콘 등은 때로는 의미를 갖기도 하지만 여기에서는 제거했습니다.
    # text = re.sub('[?.,;:|\)*~`’!^\-_+<>@\#$%&-=#}※]', '', text)
    # text = re.sub('[0-9]', '', text)
    # 한글, 영문, 숫자만 남기고 모두 제거하도록 합니다.
    # text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', ' ', text)
    # 한글, 영문만 남기고 모두 제거하도록 합니다.
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', text)
    return text
```

```
re.함수       #re.으로 시작하는 것은 정규표현식에 있는 기능이다.
```

```
sentences = art['content'].apply(preprocessing)  #apply함수를 사용해 preprocessing 함수를 적용.
```





##### 만약 워드클라우드 안에 원하는 단어수만 출력하고 싶다면:grey_question: 

```
->  row데이터에서 '상위 몇 %' 이런식으로 전처리를 해줘야 된다.
```





##  <오후수업>

###  단어 벡터화

```
띄어쓰기나 맞춤법에 따라 다른 의미 -> 그래서 텍스트 전처리가 필요!
```



### 1.텍스트 데이터 정제 및 전처리

#### 1) 데이터 정제 및 전처리

- 기계가 텍스트를 이해할 수 있도록 텍스트를 정제
- 신호와 소음을 구분
- 아웃라이어데이터로 인한 오버피팅을 방지


```
* 정규화
* 토큰화
* 어근화
* 어구추출
```



#### 2) 불용어

```
일반적으로 코퍼스에서 자주 나타나는 단어
ex) 조사, 접미사 -나, 너, 은, 는, 이, 가, 하다, 합니다 등
```



#### 3) 어간추출

```
단어를 축약형으로 바꿔준다.
ex) 새로운, 새로울 -> 새롭다
    먹었다, 먹을, 먹을지도 모르는 -> 먹다
```



#### 4) 음소표기법

##### 품사정보가 보존된 형태의 기본형으로 변환

* 배가 맛있다.
* 배를 타는 것이 재미있다.
* 평소보다 두 배로 많이 먹어서 배가 아프다.



### 2. 텍스트 데이터 벡터화

#### 1) 텍스트 데이터, 범주형 데이터 => 수치형데이터

```
- 머신러닝이나 딥러닝 알고리즘은 수치로된 데이터만 이해
```



#### 2) BOW(bag of words)

* 가장 간단하지만 효과적이라 널리쓰인다.
* 구조에 상관없이 단어의 출현횟수만 센다.
* 단어의 순서가 무시된다는 단점



#### 3) TF-IDF

``` : 
TF(단어빈도) : 특정한 단어가 문서 내에 얼마나 자주 등장하는지 나타내는 값
DF(문서빈도) /  IDF(역문서빈도)
```



#### 4) Word2Vec

```
vector size가 매우 크로 sparse하므로 neural net 성능이 잘 나오지 X
```



### 3. 텍스트 데이터 시각화

* 워드클라우드
* 청와대 국민청원 데이터
* IMDB 영화뷰 데이터
* 단어 수 혹은 문장길이, 특수문자, 불용어 갯수 등을 시각화
* 작가별 품사 사용에 대한 시각화
* 작가별 단어개수 시각화

