# 기계학습 기초 - 단어 벡터화



## 사이킷런(scikit learn)

### 1. supervised machine learning (지도학습)

- Model (training data + **training labels**) -> Prediction (test data) -> Evaluation(test labels)

  ~~~python
  """model"""
  clf = RandomForestClassifier()
  clf.fit(X_train, y_train)  # X 대문자(행렬), y 소문자(벡터)
  
  """prediction"""
  y_pred = clf.predict(X_test)	# X_train 데이터랑 X_test 데이터랑 크기 같아야됨
  """evaluation"""
  clf.score(X_test, y_test)
  ~~~

- 예 : 분류, 회귀



### 2. unsupervised machine learning (비지도학습)

- Model (training data) -> New View (test data)

  ~~~ python
  pca = PCA()
  
  """training data -> model"""
  pca.fit(X_train)
  
  """test data -> transformation"""
  X_new = pca.transform(X_test)
  ~~~



### 3. Decision Trees

- True / False 로 가지를 계속 뻗어 나가 (depth)
- 장점 : 시각화해서 보기 좋다
- 단점 : 정확도가 떨어진다



###  4. Random Forests

- decision trees의 단점을 보완
- depth가 낮으면 underfitting, depth가 높으면 overfitting 때문에 정확도가 떨어짐
- 여러 decision trees를 random하게 만들어서 fitting 정도 비교(?)





# Bag of Word Representations (BOW)

- 장, 문단, 문장, 서식과 같은 입력 텍스트의 구조를 제외하고 각 단어가 이 말뭉치에 얼마나 많이 나타나는지만 헤아림
- 구조와 상관없이 단어의 출현횟수만 셈
- 단어의 순서가 완전히 무시된다는 단점 (아래 두 문장은 의미가 전혀 반대지만 완전히 동일하게 반환)
  - it's bad, not good at all
  - it's good, not bad at all
- 이를 보완하기 위해 n-gram을 사용하는 데 bow는 하나의 토큰을 사용하지만 n-gram은 n개의 토큰을 사용
- CountVectorizer 
- TfidfVectorizer : 가중치 부여



## TF-IDF

- TF (단어 빈도, term frequency) : 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값
  - 이 값이 높을수록 문서에서 중요하다고 생각할 수 있지만, 단어 자체가 문서군 내에서 자주 사용되는 경우, 이것은 그 단어가 흔하게 등장한다는 것을 의미
- DF (문서 빈도, document frequency)
- IDF (역문서 빈도, inverse document frequency)
- TF-IDF = TF * IDF



## Word2Vec

- one hot encoding 혹은 Bag of Words에서 vector size가 매우 크고 sparse하므로 neural net 성능이 잘 나오지 않음
- 주위 단어가 비슷하면 해당 단어의 의미는 유사하다라는 아이디어
- 단어를 트레이닝 시킬 때 주변 단어를 label로 매치하여 최적화
- 단어를 의미를 내포한 dense vector로 매칭 시키는 것
- 분산된 텍스트 표현을 사용하여 개념 간 유사성을 봄

### CBOW (continuous bag-of-words)

: 전체 텍스트로 하나의 단어를 예측하기 때문에 작은 데이터셋일수록 유리하다.

### Skip-Gram

: 타겟 단어들로부터 원본 단어를 역으로 예측하는 것이다. CBOW와는 반대로 context-target 쌍을 새로운 발견으로 처리하고 큰 규모의 데이터셋을 가질 때 유리
